{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, LinearSVC,SVR\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#Imputation\n",
    "#from fancyimpute import BiScaler, KNN\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6\n",
    "\n",
    "#Create some helpers functions\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "    \n",
    "def normalize(x):\n",
    "\n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    \n",
    "    x = (x-xmin)/(xmax-xmin)\n",
    "\n",
    "    return x\n",
    "\n",
    "#def rmspe_score(y_pred,y_true):\n",
    "#    return float(np.sqrt(np.mean((1-y_pred/y_true)**2)))\n",
    "\n",
    "#def rmspe(y_pred,dtrain):\n",
    "#    labels = dtrain.get_label()\n",
    "#    return 'RMSPE',float(np.sqrt(np.mean((1-y_pred/labels)**2)))\n",
    "\n",
    "\n",
    "# Thanks to Chenglong Chen for providing this in the forum\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def metrics_rmspe(y,yhat):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_score(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe\n",
    "\n",
    "def minmaxscaler(df):\n",
    "    return \n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = df.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def percentage_annotate_helper(ax,df):\n",
    "    for p in ax.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        ax.annotate('{:.1f}%'.format(100.*y/len(df)), (x.mean(), y),\n",
    "                       ha='center', va='bottom') # set the alignment of the text\n",
    "\n",
    "def outliers_detector(df,feature):\n",
    "    Q1 = np.percentile(df[feature],25) \n",
    "    Q2 = np.percentile(df[feature],50)\n",
    "    Q3 = np.percentile(df[feature],75)\n",
    "    step = 1.5 * (Q3-Q1)\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))])\n",
    "    return df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))].index.values\n",
    "\n",
    "def plotModelResults(model, X_train, X_test,y_test,plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = rmspe_score(y_test,prediction)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(float(error[1])))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');\n",
    "    \n",
    "def modelfit(alg, dtrain, predictors,target,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            feval=rmspe, early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric=rmspe)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSPE : %.4f\" % rmspe_score(dtrain_predictions,dtrain[target].values))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.feature_importances_).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Load the data from csv\n",
    "store = pd.read_csv('./input/store.csv')\n",
    "train = pd.read_csv('./input/train.csv', parse_dates=[\"Date\"],keep_date_col=True)\n",
    "test = pd.read_csv('./input/test.csv', parse_dates=[\"Date\"],keep_date_col=True)\n",
    "\n",
    "#Data information Analysis including the data type and how many observations etc.\n",
    "display(store.info())\n",
    "display(train.info())\n",
    "\n",
    "def build_features(features):\n",
    "    #features['Promo2SinceWeek'].fillna(0,inplace=True)\n",
    "    #features['Promo2SinceYear'].fillna(0,inplace=True)\n",
    "    features['PromoInterval'].fillna('n',inplace=True)\n",
    "    #features['CompetitionOpenSinceMonth'].fillna(0,inplace=True)\n",
    "    #features['CompetitionOpenSinceYear'].fillna(0,inplace=True)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('a',1)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('b',2)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('c',3)\n",
    "    features['StateHoliday'] = features['StateHoliday'].astype(float)\n",
    "    #features['StoreType'] = features['StoreType'].replace('a',1)\n",
    "    #features['StoreType'] = features['StoreType'].replace('b',2)\n",
    "    #features['StoreType'] = features['StoreType'].replace('c',3)\n",
    "    #features['StoreType'] = features['StoreType'].replace('d',4)\n",
    "    #features['StoreType'] = features['StoreType'].astype(float)\n",
    "    #features['Assortment'] = features['Assortment'].replace('a',1)\n",
    "    #features['Assortment'] = features['Assortment'].replace('b',2)\n",
    "    #features['Assortment'] = features['Assortment'].replace('c',3)\n",
    "    #features['Assortment'] = features['Assortment'].astype(float)\n",
    "    #features['isStateHoliday'] =  features['StateHoliday'].map(lambda x: 0 if x==0 else 1)\n",
    "    #features = pd.get_dummies(features,columns=['DayOfWeek','StoreType','Assortment','PromoInterval'])\n",
    "    AssortStore = {'aa':1,'ab':2,'ac':3,'ad':4,'ba':5,'bb':6,'bc':7,'bd':8,'ca':9,'cb':10,'cc':11,'cd':12}\n",
    "    features['AssortStore'] = features['Assortment']+features['StoreType']\n",
    "    features['AssortStore'] = features['AssortStore'].map(AssortStore)\n",
    "    features['Date'] = pd.to_datetime(features['Date'])\n",
    "    \n",
    "    features['DayOfWeekPlusState'] = features['DayOfWeek'].astype(float)+features['StateHoliday']\n",
    "    features['DayOfWeekPlusSchool'] = features['DayOfWeek'].astype(float)+features['SchoolHoliday']\n",
    "    #features['DayOfWeekPlusSuperHoliday'] = features['DayOfWeek'].astype(float)+features['SchoolHoliday']+features['StateHoliday']\n",
    "    features['DayOfPromo'] = features['DayOfWeek'].astype(float)+(features['Promo'].astype(float)/2.0)\n",
    "    features['WeekOfYear'] = features['Date'].map(lambda x: x.isocalendar()[1]).astype(float)\n",
    "    features['DayOfYear'] = features['Date'].map(lambda x: x.timetuple().tm_yday).astype(float)\n",
    "    features['Day']=features['Date'].map(lambda x:x.day).astype(float)\n",
    "    features['Month'] = features['Date'].map(lambda x: x.month).astype(float)\n",
    "    features['Year'] = features['Date'].map(lambda x: x.year).astype(float)\n",
    "    features['Season'] = features['Date'].map(lambda x: 1 if x.month in [1,2,3] else (2 if x.month in [4,5,6] else (3 if x.month in [7,8,9] else 4))).astype(float)\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    features['monthStr'] = features.Month.map(month2str)\n",
    "    features.loc[features.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    features['IsPromoMonth'] = 0\n",
    "    for interval in features.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                features.loc[(features.monthStr == month) & (features.PromoInterval == interval), 'IsPromoMonth'] = 1.0\n",
    "    \n",
    "    features[\"CompetitionDistance\"].fillna(0,inplace=True)\n",
    "    features[\"CompetitionDistance\"] = np.log(features[\"CompetitionDistance\"]+1)\n",
    "    features[\"CompetitionMonthDuration\"] = (features['Month']-features['CompetitionOpenSinceMonth'])+(features['Year']-features['CompetitionOpenSinceYear'])*12\n",
    "    features[\"Promo2WeekDuration\"] = (features['WeekOfYear']-features['Promo2SinceWeek'])/4+(features['Year']-features['Promo2SinceYear'])*12\n",
    "    features['Promo2WeekDuration'] = features.Promo2WeekDuration.apply(lambda x: x if x > 0 else 0)\n",
    "    features.loc[features.Promo2SinceYear == 0, 'Promo2WeekDuration'] = 0\n",
    "    \n",
    "    features[\"CompetitionMonthDuration\"].fillna(0,inplace=True)\n",
    "    \n",
    "    features[\"Promo2WeekDuration\"].fillna(0,inplace=True)\n",
    "    features[\"CompetitionMonthDuration\"] = np.log(features[\"CompetitionMonthDuration\"]+1)\n",
    "    features[\"Promo2WeekDuration\"] = (features[\"Promo2WeekDuration\"]+1)\n",
    "    PromoCombo = {'11':1,'10':2,'01':3,'00':4}\n",
    "    features['PromoCombo'] = features['Promo'].astype(str)+features['Promo2'].astype(str)\n",
    "    features['PromoCombo'] = features['PromoCombo'].map(PromoCombo)\n",
    "    #features['AllPromo'] = features.apply(lambda x: 1 if x['Promo']&x['Promo2'] else 0,axis=1)\n",
    "    \n",
    "    features = features.drop(['Date','PromoInterval','Promo2SinceWeek','Promo2SinceYear','CompetitionOpenSinceYear','CompetitionOpenSinceMonth','monthStr','Season','Assortment','StoreType','StateHoliday','Promo2','SchoolHoliday','IsPromoMonth'],axis=1)\n",
    "    if 'Customers' in features.columns:\n",
    "        features = features.drop(['Customers'],axis=1)\n",
    "    features.fillna(0,inplace=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "#Record Feature Building Time\n",
    "feature_building_start = time.time()\n",
    "\n",
    "store['SalesPerStore'] = np.zeros(store.Store.shape)\n",
    "for i in range(1,len(store)+1):\n",
    "    avg = (train[train.Store==i][train.Sales>0]['Sales']/train[train.Store==i][train.Sales>0]['Customers']).mean()\n",
    "    store.set_value(store.Store==i,\"SalesPerStore\",avg)\n",
    "\n",
    "org_train_data = pd.merge(train,store,on='Store')\n",
    "#org_train_data['Date'] = pd.to_datetime(org_train_data['Date'])\n",
    "org_train_data = org_train_data[org_train_data['Sales']>0]\n",
    "org_train_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "org_train_data.sort_index(inplace=True)\n",
    "\n",
    "org_test_data = pd.merge(test,store,on='Store')\n",
    "#org_test_data['Date'] = pd.to_datetime(org_test_data['Date'])\n",
    "org_test_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "org_test_data.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "combine_data = org_train_data.copy()\n",
    "combine_data = org_train_data.append(org_test_data)\n",
    "\n",
    "combine_data = combine_data.copy()\n",
    "\n",
    "\n",
    "new_combine_data = pd.DataFrame([])\n",
    "min_start = 43\n",
    "max_end = 44\n",
    "\n",
    "gbm_list = []\n",
    "from datetime import datetime\n",
    "test_start_date = '2015-08-01'\n",
    "test_start_date = datetime.strptime(test_start_date,'%Y-%m-%d')\n",
    "\n",
    "org_train_data = combine_data[combine_data.Date<test_start_date].drop(['Id'],axis=1)\n",
    "org_test_data = combine_data[combine_data.Date>=test_start_date]\n",
    "\n",
    "sub_range = 10\n",
    "for s in range(1,sub_range+1):\n",
    "    small_sample = range(s,len(store)+1,sub_range)\n",
    "\n",
    "    org_train_sum_data = org_train_data[org_train_data.Store.isin(small_sample)]\n",
    "    \n",
    "\n",
    "    print('OK')\n",
    "\n",
    "    val_start_date = org_train_sum_data.iloc[-1].Date - timedelta(weeks=2)\n",
    "    mask = (org_train_sum_data['Date'] >= val_start_date) & (org_train_sum_data['Date'] <= org_train_sum_data.iloc[-1].Date)\n",
    "\n",
    "    #train_data, val_data = train_test_split(org_train_sum_data, test_size=0.012, random_state=10)\n",
    "    #split the training data and validation data\n",
    "    val_data = org_train_sum_data.loc[mask]\n",
    "    train_data = org_train_sum_data.loc[~mask]\n",
    "    #sales_train = np.log1p(org_train_data[org_train_data['Sales']!=0]['Sales'])\n",
    "    #train_data = org_train_data[org_train_data['Sales']!=0].drop('Sales',axis=1)\n",
    "    #train_data = build_features(train_data)\n",
    "\n",
    "    train_data = build_features(train_data)\n",
    "    val_data = build_features(val_data)\n",
    "    \n",
    "\n",
    "    #feature_building_elapsed = time.time() - feature_building_start\n",
    "    #print \"Feature Building %f\" % feature_building_elapsed\n",
    "\n",
    "    #display(train_data.columns)\n",
    "    display(np.array(small_sample).T)\n",
    "\n",
    "    xgboost_start = time.time()\n",
    "\n",
    "\n",
    "    features = [x for x in train_data.columns if x not in ['Sales']]\n",
    "\n",
    "    dtrain = xgb.DMatrix(train_data[features], np.log(train_data[\"Sales\"] + 1))\n",
    "    dvalid = xgb.DMatrix(val_data[features], np.log(val_data[\"Sales\"] + 1))\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "              \"booster\" : \"gbtree\",\n",
    "              \"eta\": 0.01,\n",
    "              \"max_depth\": 9,\n",
    "              \"subsample\": 0.3,\n",
    "              \"colsample_bytree\": 0.8,\n",
    "              \"min_child_weight\":8,\n",
    "              \"reg_alpha\":1e-04,\n",
    "              \"seed\":31,\n",
    "              \"silent\": 1\n",
    "              }\n",
    "    \n",
    "    num_trees = 30000\n",
    "    \n",
    "    res = {}\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=500, feval=rmspe, verbose_eval=True,evals_result=res)\n",
    "    \n",
    "    print(\"Validating\")\n",
    "    train_probs = gbm.predict(xgb.DMatrix(val_data[features]))\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe_score(np.exp(train_probs) - 1, val_data['Sales'].values)\n",
    "    print('error', error)\n",
    "\n",
    "    #from xgboost import XGBRegressor \n",
    "    #predictors = [x for x in train_data.columns]\n",
    "    #xgbr = XGBRegressor(learning_rate=0.3,n_estimators=300,silent=1,n_jobs=8,colsample_bytree=0.6,\n",
    "    #                   max_depth=6,min_child_weight=3,subsample=0.7)\n",
    "\n",
    "    #modelfit(xgbr, pd.concat([train_data,sales_train],axis=1), predictors,target='Sales')\n",
    "    xgboost_train_end = time.time() - xgboost_start\n",
    "    #print \"XGboost Trained %f\" % xgboost_train_end\n",
    "    \n",
    "    gbm_list.append(gbm)\n",
    "    \n",
    "    import operator\n",
    "\n",
    "    _,ax = plt.subplots()\n",
    "    evals_res = pd.DataFrame.from_dict(res['eval']).drop(['rmse'],axis=1)\n",
    "    train_res = pd.DataFrame.from_dict(res['train']).drop(['rmse'],axis=1)\n",
    "    x_axis = range(0,len(train_res))\n",
    "\n",
    "    ax.plot(x_axis,train_res['rmspe'],label='Train Loss')\n",
    "    ax.plot(x_axis,evals_res['rmspe'],label='Evaluation Loss')\n",
    "    ax.set_title(\"%d Error\" % small_sample[0])\n",
    "    ax.set_ylim(top=0.3)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.plot(evals_res)\n",
    "    create_feature_map(train_data[features])\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "    df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "    featp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('relative importance')\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "submission = pd.DataFrame([])\n",
    "for s,gbm in enumerate(gbm_list):\n",
    "    small_sample = range(s+1,len(store)+1,sub_range)\n",
    "    org_test_sum_data = org_test_data[org_test_data.Store.isin(small_sample)]\n",
    "    test = build_features(org_test_sum_data)\n",
    "    test_probs = gbm.predict(xgb.DMatrix(test[features]))\n",
    "    indices = test_probs < 0\n",
    "    test_probs[indices] = 0\n",
    "    not_open_Id = test[test['Open']==0].index\n",
    "    temp = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.expm1(test_probs)})\n",
    "    temp.set_value(not_open_Id,\"Sales\",0.0)\n",
    "    submission = pd.concat([submission,temp],axis=0)\n",
    "\n",
    "submission['Id'] = pd.to_numeric(submission['Id'], downcast='integer')\n",
    "submission.to_csv(\"my_submission_concat_15_eval_2.csv\", index=False)\n",
    "\n",
    "    #org_test_data['Date'] = pd.to_datetime(org_test_data['Date'])\n",
    "    #org_test_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "    #org_test_data.sort_index(inplace=True)\n",
    "    #test = build_features(org_test_data)\n",
    "    #not_open_index = test[test['Open']==0].index\n",
    "    #test_train = test.drop(['Id'],axis=1)\n",
    "    #test_result = xgbr.predict(test_train)\n",
    "    #submission = pd.DataFrame({\"Id\": test['Id'], \"Sales\": np.expm1(test_result)})\n",
    "    #submission.set_value(not_open_index,\"Sales\",0.0)\n",
    "    #submission.ix[not_open_index][\"Sales\"] = 0.0\n",
    "    #submission['Id'] = pd.to_numeric(submission['Id'], downcast='integer')\n",
    "    #display(submission.ix[not_open_index])\n",
    "    #submission.to_csv(\"my_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, LinearSVC,SVR\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#Imputation\n",
    "#from fancyimpute import BiScaler, KNN\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6\n",
    "\n",
    "#Create some helpers functions\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "    \n",
    "def normalize(x):\n",
    "\n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    \n",
    "    x = (x-xmin)/(xmax-xmin)\n",
    "\n",
    "    return x\n",
    "\n",
    "#def rmspe_score(y_pred,y_true):\n",
    "#    return float(np.sqrt(np.mean((1-y_pred/y_true)**2)))\n",
    "\n",
    "#def rmspe(y_pred,dtrain):\n",
    "#    labels = dtrain.get_label()\n",
    "#    return 'RMSPE',float(np.sqrt(np.mean((1-y_pred/labels)**2)))\n",
    "\n",
    "\n",
    "# Thanks to Chenglong Chen for providing this in the forum\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def metrics_rmspe(y,yhat):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_score(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe\n",
    "\n",
    "def minmaxscaler(df):\n",
    "    return \n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = df.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def percentage_annotate_helper(ax,df):\n",
    "    for p in ax.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        ax.annotate('{:.1f}%'.format(100.*y/len(df)), (x.mean(), y),\n",
    "                       ha='center', va='bottom') # set the alignment of the text\n",
    "\n",
    "def outliers_detector(df,feature):\n",
    "    Q1 = np.percentile(df[feature],25) \n",
    "    Q2 = np.percentile(df[feature],50)\n",
    "    Q3 = np.percentile(df[feature],75)\n",
    "    step = 1.5 * (Q3-Q1)\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))])\n",
    "    return df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))].index.values\n",
    "\n",
    "def plotModelResults(model, X_train, X_test,y_test,plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = rmspe_score(y_test,prediction)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(float(error[1])))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');\n",
    "    \n",
    "def modelfit(alg, dtrain, predictors,target,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            feval=rmspe, early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric=rmspe)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSPE : %.4f\" % rmspe_score(dtrain_predictions,dtrain[target].values))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.feature_importances_).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Load the data from csv\n",
    "store = pd.read_csv('./input/store.csv')\n",
    "train = pd.read_csv('./input/train.csv', parse_dates=[\"Date\"],keep_date_col=True)\n",
    "test = pd.read_csv('./input/test.csv', parse_dates=[\"Date\"],keep_date_col=True)\n",
    "\n",
    "#Data information Analysis including the data type and how many observations etc.\n",
    "display(store.info())\n",
    "display(train.info())\n",
    "\n",
    "def build_features(features):\n",
    "    #features['Promo2SinceWeek'].fillna(0,inplace=True)\n",
    "    #features['Promo2SinceYear'].fillna(0,inplace=True)\n",
    "    features['PromoInterval'].fillna('n',inplace=True)\n",
    "    #features['CompetitionOpenSinceMonth'].fillna(0,inplace=True)\n",
    "    #features['CompetitionOpenSinceYear'].fillna(0,inplace=True)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('a',1)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('b',2)\n",
    "    features['StateHoliday'] = features['StateHoliday'].replace('c',3)\n",
    "    features['StateHoliday'] = features['StateHoliday'].astype(float)\n",
    "    #features['StoreType'] = features['StoreType'].replace('a',1)\n",
    "    #features['StoreType'] = features['StoreType'].replace('b',2)\n",
    "    #features['StoreType'] = features['StoreType'].replace('c',3)\n",
    "    #features['StoreType'] = features['StoreType'].replace('d',4)\n",
    "    #features['StoreType'] = features['StoreType'].astype(float)\n",
    "    #features['Assortment'] = features['Assortment'].replace('a',1)\n",
    "    #features['Assortment'] = features['Assortment'].replace('b',2)\n",
    "    #features['Assortment'] = features['Assortment'].replace('c',3)\n",
    "    #features['Assortment'] = features['Assortment'].astype(float)\n",
    "    #features['isStateHoliday'] =  features['StateHoliday'].map(lambda x: 0 if x==0 else 1)\n",
    "    #features = pd.get_dummies(features,columns=['DayOfWeek','StoreType','Assortment','PromoInterval'])\n",
    "    AssortStore = {'aa':1,'ab':2,'ac':3,'ad':4,'ba':5,'bb':6,'bc':7,'bd':8,'ca':9,'cb':10,'cc':11,'cd':12}\n",
    "    features['AssortStore'] = features['Assortment']+features['StoreType']\n",
    "    features['AssortStore'] = features['AssortStore'].map(AssortStore)\n",
    "    features['Date'] = pd.to_datetime(features['Date'])\n",
    "    \n",
    "    features['DayOfWeekPlusState'] = features['DayOfWeek'].astype(float)+features['StateHoliday']\n",
    "    features['DayOfWeekPlusSchool'] = features['DayOfWeek'].astype(float)+features['SchoolHoliday']\n",
    "    #features['DayOfWeekPlusSuperHoliday'] = features['DayOfWeek'].astype(float)+features['SchoolHoliday']+features['StateHoliday']\n",
    "    features['DayOfPromo'] = features['DayOfWeek'].astype(float)+(features['Promo'].astype(float)/2.0)\n",
    "    features['WeekOfYear'] = features['Date'].map(lambda x: x.isocalendar()[1]).astype(float)\n",
    "    features['DayOfYear'] = features['Date'].map(lambda x: x.timetuple().tm_yday).astype(float)\n",
    "    features['Day']=features['Date'].map(lambda x:x.day).astype(float)\n",
    "    features['Month'] = features['Date'].map(lambda x: x.month).astype(float)\n",
    "    features['Year'] = features['Date'].map(lambda x: x.year).astype(float)\n",
    "    features['Season'] = features['Date'].map(lambda x: 1 if x.month in [1,2,3] else (2 if x.month in [4,5,6] else (3 if x.month in [7,8,9] else 4))).astype(float)\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    features['monthStr'] = features.Month.map(month2str)\n",
    "    features.loc[features.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    features['IsPromoMonth'] = 0\n",
    "    for interval in features.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                features.loc[(features.monthStr == month) & (features.PromoInterval == interval), 'IsPromoMonth'] = 1.0\n",
    "    \n",
    "    features[\"CompetitionDistance\"].fillna(0,inplace=True)\n",
    "    features[\"CompetitionDistance\"] = np.log(features[\"CompetitionDistance\"]+1)\n",
    "    features[\"CompetitionMonthDuration\"] = (features['Month']-features['CompetitionOpenSinceMonth'])+(features['Year']-features['CompetitionOpenSinceYear'])*12\n",
    "    features[\"Promo2WeekDuration\"] = (features['WeekOfYear']-features['Promo2SinceWeek'])/4+(features['Year']-features['Promo2SinceYear'])*12\n",
    "    features['Promo2WeekDuration'] = features.Promo2WeekDuration.apply(lambda x: x if x > 0 else 0)\n",
    "    features.loc[features.Promo2SinceYear == 0, 'Promo2WeekDuration'] = 0\n",
    "    \n",
    "    features[\"CompetitionMonthDuration\"].fillna(0,inplace=True)\n",
    "    \n",
    "    features[\"Promo2WeekDuration\"].fillna(0,inplace=True)\n",
    "    features[\"CompetitionMonthDuration\"] = np.log(features[\"CompetitionMonthDuration\"]+1)\n",
    "    features[\"Promo2WeekDuration\"] = (features[\"Promo2WeekDuration\"]+1)\n",
    "    PromoCombo = {'11':1,'10':2,'01':3,'00':4}\n",
    "    features['PromoCombo'] = features['Promo'].astype(str)+features['Promo2'].astype(str)\n",
    "    features['PromoCombo'] = features['PromoCombo'].map(PromoCombo)\n",
    "    #features['AllPromo'] = features.apply(lambda x: 1 if x['Promo']&x['Promo2'] else 0,axis=1)\n",
    "    \n",
    "    features = features.drop(['Date','PromoInterval','Promo2SinceWeek','Promo2SinceYear','CompetitionOpenSinceYear','CompetitionOpenSinceMonth','monthStr','Season','Assortment','StoreType','StateHoliday','Promo2','SchoolHoliday','IsPromoMonth'],axis=1)\n",
    "    if 'Customers' in features.columns:\n",
    "        features = features.drop(['Customers'],axis=1)\n",
    "    features.fillna(0,inplace=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "#Record Feature Building Time\n",
    "feature_building_start = time.time()\n",
    "\n",
    "store['SalesPerStore'] = np.zeros(store.Store.shape)\n",
    "for i in range(1,len(store)+1):\n",
    "    avg = (train[train.Store==i][train.Sales>0]['Sales']/train[train.Store==i][train.Sales>0]['Customers']).mean()\n",
    "    store.set_value(store.Store==i,\"SalesPerStore\",avg)\n",
    "\n",
    "org_train_data = pd.merge(train,store,on='Store')\n",
    "#org_train_data['Date'] = pd.to_datetime(org_train_data['Date'])\n",
    "org_train_data = org_train_data[org_train_data['Sales']>0]\n",
    "org_train_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "org_train_data.sort_index(inplace=True)\n",
    "\n",
    "org_test_data = pd.merge(test,store,on='Store')\n",
    "#org_test_data['Date'] = pd.to_datetime(org_test_data['Date'])\n",
    "org_test_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "org_test_data.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "combine_data = org_train_data.copy()\n",
    "combine_data = org_train_data.append(org_test_data)\n",
    "\n",
    "combine_data = combine_data.copy()\n",
    "\n",
    "\n",
    "new_combine_data = pd.DataFrame([])\n",
    "min_start = 43\n",
    "max_end = 44\n",
    "\n",
    "gbm_list = []\n",
    "from datetime import datetime\n",
    "test_start_date = '2015-08-01'\n",
    "test_start_date = datetime.strptime(test_start_date,'%Y-%m-%d')\n",
    "\n",
    "org_train_data = combine_data[combine_data.Date<test_start_date].drop(['Id'],axis=1)\n",
    "org_test_data = combine_data[combine_data.Date>=test_start_date]\n",
    "\n",
    "sub_range = 5\n",
    "for s in range(1,sub_range+1):\n",
    "    small_sample = range(s,len(store)+1,sub_range)\n",
    "\n",
    "    org_train_sum_data = org_train_data[org_train_data.Store.isin(small_sample)]\n",
    "    \n",
    "\n",
    "    print('OK')\n",
    "\n",
    "    val_start_date = org_train_sum_data.iloc[-1].Date - timedelta(weeks=4)\n",
    "    mask = (org_train_sum_data['Date'] >= val_start_date) & (org_train_sum_data['Date'] <= org_train_sum_data.iloc[-1].Date)\n",
    "\n",
    "    #train_data, val_data = train_test_split(org_train_sum_data, test_size=0.012, random_state=10)\n",
    "    #split the training data and validation data\n",
    "    val_data = org_train_sum_data.loc[mask]\n",
    "    train_data = org_train_sum_data.loc[~mask]\n",
    "    #sales_train = np.log1p(org_train_data[org_train_data['Sales']!=0]['Sales'])\n",
    "    #train_data = org_train_data[org_train_data['Sales']!=0].drop('Sales',axis=1)\n",
    "    #train_data = build_features(train_data)\n",
    "\n",
    "    train_data = build_features(train_data)\n",
    "    val_data = build_features(val_data)\n",
    "    \n",
    "\n",
    "    #feature_building_elapsed = time.time() - feature_building_start\n",
    "    #print \"Feature Building %f\" % feature_building_elapsed\n",
    "\n",
    "    #display(train_data.columns)\n",
    "    display(np.array(small_sample).T)\n",
    "\n",
    "    xgboost_start = time.time()\n",
    "\n",
    "\n",
    "    features = [x for x in train_data.columns if x not in ['Sales']]\n",
    "\n",
    "    dtrain = xgb.DMatrix(train_data[features], np.log(train_data[\"Sales\"] + 1))\n",
    "    dvalid = xgb.DMatrix(val_data[features], np.log(val_data[\"Sales\"] + 1))\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "              \"booster\" : \"gbtree\",\n",
    "              \"eta\": 0.01,\n",
    "              \"max_depth\": 9,\n",
    "              \"subsample\": 0.3,\n",
    "              \"colsample_bytree\": 0.8,\n",
    "              \"min_child_weight\":8,\n",
    "              \"reg_alpha\":1e-04,\n",
    "              \"seed\":31,\n",
    "              \"silent\": 1\n",
    "              }\n",
    "    \n",
    "    num_trees = 30000\n",
    "    \n",
    "    res = {}\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=500, feval=rmspe, verbose_eval=True,evals_result=res)\n",
    "    \n",
    "    print(\"Validating\")\n",
    "    train_probs = gbm.predict(xgb.DMatrix(val_data[features]))\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe_score(np.exp(train_probs) - 1, val_data['Sales'].values)\n",
    "    print('error', error)\n",
    "\n",
    "    #from xgboost import XGBRegressor \n",
    "    #predictors = [x for x in train_data.columns]\n",
    "    #xgbr = XGBRegressor(learning_rate=0.3,n_estimators=300,silent=1,n_jobs=8,colsample_bytree=0.6,\n",
    "    #                   max_depth=6,min_child_weight=3,subsample=0.7)\n",
    "\n",
    "    #modelfit(xgbr, pd.concat([train_data,sales_train],axis=1), predictors,target='Sales')\n",
    "    xgboost_train_end = time.time() - xgboost_start\n",
    "    #print \"XGboost Trained %f\" % xgboost_train_end\n",
    "    \n",
    "    gbm_list.append(gbm)\n",
    "    \n",
    "    import operator\n",
    "\n",
    "    _,ax = plt.subplots()\n",
    "    evals_res = pd.DataFrame.from_dict(res['eval']).drop(['rmse'],axis=1)\n",
    "    train_res = pd.DataFrame.from_dict(res['train']).drop(['rmse'],axis=1)\n",
    "    x_axis = range(0,len(train_res))\n",
    "\n",
    "    ax.plot(x_axis,train_res['rmspe'],label='Train Loss')\n",
    "    ax.plot(x_axis,evals_res['rmspe'],label='Evaluation Loss')\n",
    "    ax.set_title(\"%d Error\" % small_sample[0])\n",
    "    ax.set_ylim(top=0.3)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.plot(evals_res)\n",
    "    create_feature_map(train_data[features])\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "    df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "    featp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('relative importance')\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "submission = pd.DataFrame([])\n",
    "for s,gbm in enumerate(gbm_list):\n",
    "    small_sample = range(s+1,len(store)+1,sub_range)\n",
    "    org_test_sum_data = org_test_data[org_test_data.Store.isin(small_sample)]\n",
    "    test = build_features(org_test_sum_data)\n",
    "    test_probs = gbm.predict(xgb.DMatrix(test[features]))\n",
    "    indices = test_probs < 0\n",
    "    test_probs[indices] = 0\n",
    "    not_open_Id = test[test['Open']==0].index\n",
    "    temp = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.expm1(test_probs)})\n",
    "    temp.set_value(not_open_Id,\"Sales\",0.0)\n",
    "    submission = pd.concat([submission,temp],axis=0)\n",
    "\n",
    "submission['Id'] = pd.to_numeric(submission['Id'], downcast='integer')\n",
    "submission.to_csv(\"my_submission_concat_8_eval_4.csv\", index=False)\n",
    "\n",
    "    #org_test_data['Date'] = pd.to_datetime(org_test_data['Date'])\n",
    "    #org_test_data.set_index(['Date','Store'],inplace=True,drop=False)\n",
    "    #org_test_data.sort_index(inplace=True)\n",
    "    #test = build_features(org_test_data)\n",
    "    #not_open_index = test[test['Open']==0].index\n",
    "    #test_train = test.drop(['Id'],axis=1)\n",
    "    #test_result = xgbr.predict(test_train)\n",
    "    #submission = pd.DataFrame({\"Id\": test['Id'], \"Sales\": np.expm1(test_result)})\n",
    "    #submission.set_value(not_open_index,\"Sales\",0.0)\n",
    "    #submission.ix[not_open_index][\"Sales\"] = 0.0\n",
    "    #submission['Id'] = pd.to_numeric(submission['Id'], downcast='integer')\n",
    "    #display(submission.ix[not_open_index])\n",
    "    #submission.to_csv(\"my_submission.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
